{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(dataset_dict):\n",
    "\t# Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n",
    "\tdataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "\timage = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "\timage, transforms = T.apply_transform_gens([T.Resize((800, 800))], image)\n",
    "\tdataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "\tannos = [\n",
    "\t\tutils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "\t\tfor obj in dataset_dict.pop(\"annotations\")\n",
    "\t\tif obj.get(\"iscrowd\", 0) == 0\n",
    "\t]\n",
    "\tinstances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "\tdataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\treturn dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = build_detection_train_loader(cfg, mapper=mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachikadoTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "    \n",
    "    # これをオーバライドする\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "#         return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        return build_detection_train_loader(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from fvcore.common.file_io import PathManager\n",
    "from PIL import Image\n",
    "\n",
    "from detectron2.data import detection_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMapper:\n",
    "    \"\"\"\n",
    "    A callable which takes a dataset dict in Detectron2 Dataset format,\n",
    "    and map it into a format used by the model.\n",
    "\n",
    "    This is the default callable to be used to map your dataset dict into training data.\n",
    "    You may need to follow it to implement your own one for customized logic,\n",
    "    such as a different way to read or transform images.\n",
    "    See :doc:`/tutorials/data_loading` for details.\n",
    "\n",
    "    The callable currently does the following:\n",
    "\n",
    "    1. Read the image from \"file_name\"\n",
    "    2. Applies cropping/geometric transforms to the image and annotations\n",
    "    3. Prepare data and annotations to Tensor and :class:`Instances`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, is_train=True):\n",
    "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
    "            self.crop_gen = T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)\n",
    "            logging.getLogger(__name__).info(\"CropGen used in training: \" + str(self.crop_gen))\n",
    "        else:\n",
    "            self.crop_gen = None\n",
    "\n",
    "        self.tfm_gens = utils.build_transform_gen(cfg, is_train)\n",
    "\n",
    "        # fmt: off\n",
    "        self.img_format     = cfg.INPUT.FORMAT\n",
    "        self.mask_on        = cfg.MODEL.MASK_ON\n",
    "        self.mask_format    = cfg.INPUT.MASK_FORMAT\n",
    "        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON\n",
    "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
    "        # fmt: on\n",
    "        if self.keypoint_on and is_train:\n",
    "            # Flip only makes sense in training\n",
    "            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)\n",
    "        else:\n",
    "            self.keypoint_hflip_indices = None\n",
    "\n",
    "        if self.load_proposals:\n",
    "            self.min_box_side_len = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE\n",
    "            self.proposal_topk = (\n",
    "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
    "                if is_train\n",
    "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
    "            )\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n",
    "\n",
    "        Returns:\n",
    "            dict: a format that builtin models in detectron2 accept\n",
    "        \"\"\"\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        # USER: Write your own image loading if it's not from a file\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
    "        utils.check_image_size(dataset_dict, image)\n",
    "\n",
    "        if \"annotations\" not in dataset_dict:\n",
    "            image, transforms = T.apply_transform_gens(\n",
    "                ([self.crop_gen] if self.crop_gen else []) + self.tfm_gens, image\n",
    "            )\n",
    "        else:\n",
    "            # Crop around an instance if there are instances in the image.\n",
    "            # USER: Remove if you don't use cropping\n",
    "            if self.crop_gen:\n",
    "                crop_tfm = utils.gen_crop_transform_with_instance(\n",
    "                    self.crop_gen.get_crop_size(image.shape[:2]),\n",
    "                    image.shape[:2],\n",
    "                    np.random.choice(dataset_dict[\"annotations\"]),\n",
    "                )\n",
    "                image = crop_tfm.apply_image(image)\n",
    "            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n",
    "            if self.crop_gen:\n",
    "                transforms = crop_tfm + transforms\n",
    "\n",
    "        image_shape = image.shape[:2]  # h, w\n",
    "\n",
    "        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n",
    "        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n",
    "        # Therefore it's important to use torch.Tensor.\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "\n",
    "        # USER: Remove if you don't use pre-computed proposals.\n",
    "        if self.load_proposals:\n",
    "            utils.transform_proposals(\n",
    "                dataset_dict, image_shape, transforms, self.min_box_side_len, self.proposal_topk\n",
    "            )\n",
    "\n",
    "        if not self.is_train:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            dataset_dict.pop(\"annotations\", None)\n",
    "            dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "            return dataset_dict\n",
    "\n",
    "        if \"annotations\" in dataset_dict:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            for anno in dataset_dict[\"annotations\"]:\n",
    "                if not self.mask_on:\n",
    "                    anno.pop(\"segmentation\", None)\n",
    "                if not self.keypoint_on:\n",
    "                    anno.pop(\"keypoints\", None)\n",
    "\n",
    "            # USER: Implement additional transformations if you have other types of data\n",
    "            annos = [\n",
    "                utils.transform_instance_annotations(\n",
    "                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n",
    "                )\n",
    "                for obj in dataset_dict.pop(\"annotations\")\n",
    "                if obj.get(\"iscrowd\", 0) == 0\n",
    "            ]\n",
    "            instances = utils.annotations_to_instances(\n",
    "                annos, image_shape, mask_format=self.mask_format\n",
    "            )\n",
    "            # Create a tight bounding box from masks, useful when image is cropped\n",
    "            if self.crop_gen and instances.has(\"gt_masks\"):\n",
    "                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n",
    "            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\n",
    "        # USER: Remove if you don't do semantic/panoptic segmentation.\n",
    "        if \"sem_seg_file_name\" in dataset_dict:\n",
    "            with PathManager.open(dataset_dict.pop(\"sem_seg_file_name\"), \"rb\") as f:\n",
    "                sem_seg_gt = Image.open(f)\n",
    "                sem_seg_gt = np.asarray(sem_seg_gt, dtype=\"uint8\")\n",
    "            sem_seg_gt = transforms.apply_segmentation(sem_seg_gt)\n",
    "            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n",
    "            dataset_dict[\"sem_seg\"] = sem_seg_gt\n",
    "        return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
