{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 続・detectron2 for まちカドまぞく ～訓練編～\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/33882378/79055210-2ff0e600-7c86-11ea-93c6-8a65112f80f0.jpg\">\n",
    "\n",
    "detectron2 の訓練をカスタマイズする方法\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VoTT のエクスポートファイルや、画像が格納されているディレクトリ\n",
    "BASE_DIRECTORY = './vott-json-export/'\n",
    "# VoTT のエクスポートファイル名\n",
    "EXPORT_FILENAME = 'Machikado-export.json'\n",
    "# 訓練データに使用する割合\n",
    "TRAIN_RATIO = 0.8\n",
    "# 乱数シード\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetCatalogを用意する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告: name: 59.jpg - 画像サイズの不整合 image_size:(268, 201), ./vott-json-export/Machikado-export.json: (600, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metadata(name='test', thing_classes=['Shamiko', 'Gosenzo', 'Lilith', 'Momo', 'Mikan', 'Mob'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from Machikado_vott import get_cat_names, get_machikado_dicts\n",
    "\n",
    "# vott エクスポートファイルの読み込み\n",
    "CAT_NAME2ID, CAT_ID2NAME = get_cat_names(os.path.join(BASE_DIRECTORY, EXPORT_FILENAME))\n",
    "dataset_dicts = get_machikado_dicts(os.path.join(BASE_DIRECTORY, EXPORT_FILENAME), BASE_DIRECTORY, CAT_NAME2ID)\n",
    "\n",
    "# 訓練用、テスト用に分ける\n",
    "random.seed(RANDOM_STATE)\n",
    "random.shuffle(dataset_dicts)\n",
    "\n",
    "split_idx = int(len(dataset_dicts) * TRAIN_RATIO) + 1\n",
    "\n",
    "# 登録\n",
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register('train', lambda : dataset_dicts[:split_idx])\n",
    "DatasetCatalog.register('test', lambda : dataset_dicts[split_idx:])\n",
    "\n",
    "MetadataCatalog.get('train').set(thing_classes=list(CAT_NAME2ID.keys()))\n",
    "MetadataCatalog.get('test').set(thing_classes=list(CAT_NAME2ID.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog\n",
    "\n",
    "# 訓練用、テスト用に分ける\n",
    "random.seed(RANDOM_STATE)\n",
    "random.shuffle(dataset_dicts)\n",
    "split_idx = int(len(dataset_dicts) * TRAIN_RATIO) + 1\n",
    "\n",
    "# 登録\n",
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register('train', lambda : dataset_dicts[:split_idx])\n",
    "DatasetCatalog.register('test', lambda : dataset_dicts[split_idx:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 学習\n",
    "\n",
    "### カスタムデータマッパー\n",
    "\n",
    "オリジナルの DatasetMapper の位置は detectron2/data/dataset_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from fvcore.common.file_io import PathManager\n",
    "from PIL import Image\n",
    "\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "class MachikadoDatasetMapper:\n",
    "    def __init__(self, cfg, is_train=True):\n",
    "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
    "            self.crop_gen = T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)\n",
    "            logging.getLogger(__name__).info(\"CropGen used in training: \" + str(self.crop_gen))\n",
    "        else:\n",
    "            self.crop_gen = None\n",
    "\n",
    "        self.tfm_gens = utils.build_transform_gen(cfg, is_train)\n",
    "\n",
    "        # fmt: off\n",
    "        self.img_format     = cfg.INPUT.FORMAT\n",
    "        self.mask_on        = cfg.MODEL.MASK_ON\n",
    "        self.mask_format    = cfg.INPUT.MASK_FORMAT\n",
    "        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON\n",
    "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
    "        # fmt: on\n",
    "        if self.keypoint_on and is_train:\n",
    "            # Flip only makes sense in training\n",
    "            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)\n",
    "        else:\n",
    "            self.keypoint_hflip_indices = None\n",
    "\n",
    "        if self.load_proposals:\n",
    "            self.min_box_side_len = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE\n",
    "            self.proposal_topk = (\n",
    "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
    "                if is_train\n",
    "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
    "            )\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        # USER: Write your own image loading if it's not from a file\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
    "        utils.check_image_size(dataset_dict, image)\n",
    "\n",
    "        if \"annotations\" not in dataset_dict:\n",
    "            image, transforms = T.apply_transform_gens(\n",
    "                ([self.crop_gen] if self.crop_gen else []) + self.tfm_gens, image\n",
    "            )\n",
    "        else:\n",
    "            # Crop around an instance if there are instances in the image.\n",
    "            # USER: Remove if you don't use cropping\n",
    "            if self.crop_gen:\n",
    "                crop_tfm = utils.gen_crop_transform_with_instance(\n",
    "                    self.crop_gen.get_crop_size(image.shape[:2]),\n",
    "                    image.shape[:2],\n",
    "                    np.random.choice(dataset_dict[\"annotations\"]),\n",
    "                )\n",
    "                image = crop_tfm.apply_image(image)\n",
    "            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n",
    "            if self.crop_gen:\n",
    "                transforms = crop_tfm + transforms\n",
    "\n",
    "        image_shape = image.shape[:2]  # h, w\n",
    "\n",
    "        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n",
    "        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n",
    "        # Therefore it's important to use torch.Tensor.\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "\n",
    "        # USER: Remove if you don't use pre-computed proposals.\n",
    "        if self.load_proposals:\n",
    "            utils.transform_proposals(\n",
    "                dataset_dict, image_shape, transforms, self.min_box_side_len, self.proposal_topk\n",
    "            )\n",
    "\n",
    "        if not self.is_train:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            dataset_dict.pop(\"annotations\", None)\n",
    "            dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "            return dataset_dict\n",
    "\n",
    "        if \"annotations\" in dataset_dict:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            for anno in dataset_dict[\"annotations\"]:\n",
    "                if not self.mask_on:\n",
    "                    anno.pop(\"segmentation\", None)\n",
    "                if not self.keypoint_on:\n",
    "                    anno.pop(\"keypoints\", None)\n",
    "\n",
    "            # USER: Implement additional transformations if you have other types of data\n",
    "            annos = [\n",
    "                utils.transform_instance_annotations(\n",
    "                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n",
    "                )\n",
    "                for obj in dataset_dict.pop(\"annotations\")\n",
    "                if obj.get(\"iscrowd\", 0) == 0\n",
    "            ]\n",
    "            instances = utils.annotations_to_instances(\n",
    "                annos, image_shape, mask_format=self.mask_format\n",
    "            )\n",
    "            # Create a tight bounding box from masks, useful when image is cropped\n",
    "            if self.crop_gen and instances.has(\"gt_masks\"):\n",
    "                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n",
    "            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\n",
    "        # USER: Remove if you don't do semantic/panoptic segmentation.\n",
    "        if \"sem_seg_file_name\" in dataset_dict:\n",
    "            with PathManager.open(dataset_dict.pop(\"sem_seg_file_name\"), \"rb\") as f:\n",
    "                sem_seg_gt = Image.open(f)\n",
    "                sem_seg_gt = np.asarray(sem_seg_gt, dtype=\"uint8\")\n",
    "            sem_seg_gt = transforms.apply_segmentation(sem_seg_gt)\n",
    "            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n",
    "            dataset_dict[\"sem_seg\"] = sem_seg_gt\n",
    "        return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カスタムトレイナー\n",
    "\n",
    "DefaultTrainer の位置は detectron2/engine/defaults.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "from detectron2.data import (\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "\n",
    "class MachikadoTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "    \n",
    "    # これをオーバライドする\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "#         return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        print('OK!!')\n",
    "        return build_detection_train_loader(cfg, MachikadoDatasetMapper(cfg, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.OUTPUT_DIR = './output'\n",
    "cfg.CUDA = 'cuda:0'\n",
    "\n",
    "# cfg.merge_from_file(\"../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "# cfg.MODEL.WEIGHTS = './coco_models/model_final_f10217.pkl'\n",
    "# cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "\n",
    "# 重いけど、これ精度良いです。\n",
    "cfg.merge_from_file('../configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml')\n",
    "cfg.MODEL.WEIGHTS = './coco_models/model_final_2d9806.pkl'\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1 # GTX2070 ではこれが限界\n",
    "\n",
    "cfg.DATASETS.TRAIN = ('train',)\n",
    "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 10    # 300 iterations seems good enough, but you can certainly train longer <- とあるが、まあデータセットによるよね・・・\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(CAT_ID2NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!!\n",
      "\u001b[32m[04/12 11:48:19 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[04/12 11:48:19 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 57 images left.\n",
      "\u001b[32m[04/12 11:48:19 d2.data.build]: \u001b[0mDistribution of instances among all 6 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|  Shamiko   | 38           |  Gosenzo   | 20           |   Lilith   | 10           |\n",
      "|    Momo    | 20           |   Mikan    | 8            |    Mob     | 8            |\n",
      "|            |              |            |              |            |              |\n",
      "|   total    | 104          |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[04/12 11:48:19 d2.data.common]: \u001b[0mSerializing 57 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/12 11:48:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.12 MiB\n",
      "\u001b[32m[04/12 11:48:19 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'roi_heads.box_predictor.cls_score.weight' has shape (81, 1024) in the checkpoint but (7, 1024) in the model! Skipped.\n",
      "'roi_heads.box_predictor.cls_score.bias' has shape (81,) in the checkpoint but (7,) in the model! Skipped.\n",
      "'roi_heads.box_predictor.bbox_pred.weight' has shape (320, 1024) in the checkpoint but (24, 1024) in the model! Skipped.\n",
      "'roi_heads.box_predictor.bbox_pred.bias' has shape (320,) in the checkpoint but (24,) in the model! Skipped.\n",
      "'roi_heads.mask_head.predictor.weight' has shape (80, 256, 1, 1) in the checkpoint but (6, 256, 1, 1) in the model! Skipped.\n",
      "'roi_heads.mask_head.predictor.bias' has shape (80,) in the checkpoint but (6,) in the model! Skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/12 11:48:21 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[04/12 11:48:34 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 3.374  loss_cls: 1.948  loss_box_reg: 0.739  loss_mask: 0.691  loss_rpn_cls: 0.010  loss_rpn_loc: 0.022  time: 0.4708  data_time: 0.0096  lr: 0.000002  max_mem: 3690M\n",
      "\u001b[32m[04/12 11:48:34 d2.engine.hooks]: \u001b[0mOverall training speed: 7 iterations in 0:00:03 (0.5381 s / it)\n",
      "\u001b[32m[04/12 11:48:34 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:12 (0:00:08 on hooks)\n"
     ]
    }
   ],
   "source": [
    "# 出力先のディレクトリを作る\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "trainer = MachikadoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False) # True で途中から学習できるらしい\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
